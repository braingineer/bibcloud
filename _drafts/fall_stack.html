---
title: fallstack2016; from arxiv
layout: default
categories: bibs
date: 2016-10-26
description: automatically generated from arxiv2bib.py
---




<div id='bibitems'><div class='btn-container'><br><input class='search' placeholder='Search title, authors, abstract, and notes'/><br><button class='sort' data-sort='author'>Sort by Author</button><button class='sort' data-sort='year'>Sort by Year</button><button class='sort' data-sort='title'>Sort by Title</button><button id='showhide'>Expand all annotations</button></div>
<ul class='bib list'><li>
                  <div id='entry0' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.00777v1' class='title'>End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>
                    <br><span class='author'>Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation0hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno0' class='bib annotation'>
                          <div id='abstract0' class='bib abstract'><b>Abstract</b>
                            This paper proposes \emph{KB-InfoBot}---a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced "soft" posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KB-InfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.</div>
                          <div id='notes0' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry1' class='bib entry'>

                    <a href='http://arxiv.org/abs/1608.05813v3' class='title'>phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning</a>
                    <br><span class='author'>Ying Hua Tan, Chee Seng Chan.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation1hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno1' class='bib annotation'>
                          <div id='abstract1' class='bib abstract'><b>Abstract</b>
                            A picture is worth a thousand words. Not until recently, however, we noticed some success stories in understanding of visual scenes: a model that is able to detect/name objects, describe their attributes, and recognize their relationships/interactions. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. The proposed model encodes sentence as a sequence of combination of phrases and words, instead of a sequence of words alone as in those conventional solutions. The two levels of this model are dedicated to i) learn to generate image relevant noun phrases, and ii) produce appropriate image description from the phrases and other words in the corpus. Adopting a convolutional neural network to learn image features and the LSTM to learn the word sequence in a sentence, the proposed model has shown better or competitive results in comparison to the state-of-the-art models on Flickr8k and Flickr30k datasets.</div>
                          <div id='notes1' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry2' class='bib entry'>

                    <a href='http://arxiv.org/abs/1606.03632v2' class='title'>Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data</a>
                    <br><span class='author'>Shikhar Sharma, Jing He, Kaheer Suleman, Hannes Schulz, Philip Bachman.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation2hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno2' class='bib annotation'>
                          <div id='abstract2' class='bib abstract'><b>Abstract</b>
                            Natural language generation plays a critical role in any spoken dialogue system. We present a new approach to natural language generation using recurrent neural networks in an encoder-decoder framework. In contrast with previous work, our model uses both lexicalized and delexicalized versions of slot-value pairs for each dialogue act. This allows our model to learn from all available data, rather than being restricted to learning only from delexicalized slot-value pairs. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by initializing its weights from a pretrained language model. Human evaluation of our best-performing model indicates that it generates sentences which users find more natural and appealing.</div>
                          <div id='notes2' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry3' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.03286v1' class='title'>Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks</a>
                    <br><span class='author'>Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation3hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno3' class='bib annotation'>
                          <div id='abstract3' class='bib abstract'><b>Abstract</b>
                            Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.</div>
                          <div id='notes3' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry4' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.03441v1' class='title'>Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing</a>
                    <br><span class='author'>Jan Chorowski, Michał Zapotoczny, Paweł Rychlikowski.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation4hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno4' class='bib annotation'>
                          <div id='abstract4' class='bib abstract'><b>Abstract</b>
                            We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn't require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transition- based system trained on perfect POS tags.</div>
                          <div id='notes4' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry5' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.03528v1' class='title'>The Microsoft 2016 Conversational Speech Recognition System</a>
                    <br><span class='author'>W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation5hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno5' class='bib annotation'>
                          <div id='abstract5' class='bib abstract'><b>Abstract</b>
                            We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task.</div>
                          <div id='notes5' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry6' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.07451v1' class='title'>AMR-to-text generation as a Traveling Salesman Problem</a>
                    <br><span class='author'>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, Daniel Gildea.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation6hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno6' class='bib annotation'>
                          <div id='abstract6' class='bib abstract'><b>Abstract</b>
                            The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.</div>
                          <div id='notes6' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry7' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02424v1' class='title'>Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</a>
                    <br><span class='author'>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation7hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno7' class='bib annotation'>
                          <div id='abstract7' class='bib abstract'><b>Abstract</b>
                            Neural sequence models are widely used to model time-series data in many fields. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose \emph{Diverse Beam Search} (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a \emph{better search algorithm}. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.</div>
                          <div id='notes7' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry8' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02544v1' class='title'>Computational linking theory</a>
                    <br><span class='author'>Aaron Steven White, Drew Reisinger, Rachel Rudinger, Kyle Rawlins, Benjamin Van Durme.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation8hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno8' class='bib annotation'>
                          <div id='abstract8' class='bib abstract'><b>Abstract</b>
                            A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes.</div>
                          <div id='notes8' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry9' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02692v1' class='title'>Open-Ended Visual Question-Answering</a>
                    <br><span class='author'>Issey Masuda, Santiago Pascual de la Puente, Xavier Giro-i-Nieto.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation9hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno9' class='bib annotation'>
                          <div id='abstract9' class='bib abstract'><b>Abstract</b>
                            This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations.</div>
                          <div id='notes9' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry10' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02806v1' class='title'>Modelling Sentence Pairs with Tree-structured Attentive Encoder</a>
                    <br><span class='author'>Yao Zhou, Cong Liu, Yan Pan.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation10hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno10' class='bib annotation'>
                          <div id='abstract10' class='bib abstract'><b>Abstract</b>
                            We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.</div>
                          <div id='notes10' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry11' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02891v2' class='title'>Personalizing a Dialogue System with Transfer Learning</a>
                    <br><span class='author'>Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation11hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno11' class='bib annotation'>
                          <div id='abstract11' class='bib abstract'><b>Abstract</b>
                            It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose "PETAL"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.</div>
                          <div id='notes11' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry12' class='bib entry'>

                    <a href='http://arxiv.org/abs/1311.3011v2' class='title'>Cornell SPF: Cornell Semantic Parsing Framework</a>
                    <br><span class='author'>Yoav Artzi.</span> <br>
                    <span class='year'>2013</span><br>
                    <div id='annotation12hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno12' class='bib annotation'>
                          <div id='abstract12' class='bib abstract'><b>Abstract</b>
                            The Cornell Semantic Parsing Framework (SPF) is a learning and inference framework for mapping natural language to formal representation of its meaning.</div>
                          <div id='notes12' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry13' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.05511v3' class='title'>Multilinear Grammar: Ranks and Interpretations</a>
                    <br><span class='author'>Dafydd Gibbon, Sascha Griffiths.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation13hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno13' class='bib annotation'>
                          <div id='abstract13' class='bib abstract'><b>Abstract</b>
                            Multilinear Grammar (MLG) is an approach to integrating the many different syntagmatic structures of language into a coherent architecture, the Rank-Interpretation Architecture. The architecture defines ranks from discourse structure through utterances, phrasal structures, word structures to speech sounds. Each rank has its own specific kind of prosodic-phonetic interpretation and semantic-pragmatic interpretation. Common to models of all these subdomains are models based on regular languages, and processors with finite working memory.</div>
                          <div id='notes13' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry14' class='bib entry'>

                    <a href='http://arxiv.org/abs/1602.07776v3' class='title'>Recurrent Neural Network Grammars</a>
                    <br><span class='author'>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation14hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno14' class='bib annotation'>
                          <div id='abstract14' class='bib abstract'><b>Abstract</b>
                            We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.</div>
                          <div id='notes14' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry15' class='bib entry'>

                    <a href='http://arxiv.org/abs/1604.00562v2' class='title'>Reasoning About Pragmatics with Neural Listeners and Speakers</a>
                    <br><span class='author'>Jacob Andreas, Dan Klein.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation15hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno15' class='bib annotation'>
                          <div id='abstract15' class='bib abstract'><b>Abstract</b>
                            We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural "listener" and "speaker" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.</div>
                          <div id='notes15' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry16' class='bib entry'>

                    <a href='http://arxiv.org/abs/1609.08777v1' class='title'>Character Sequence Models for ColorfulWords</a>
                    <br><span class='author'>Kazuya Kawakami, Chris Dyer, Bryan R. Routledge, Noah A. Smith.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation16hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno16' class='bib annotation'>
                          <div id='abstract16' class='bib abstract'><b>Abstract</b>
                            We present a neural network architecture to predict a point in color space from the sequence of characters in the color's name. Using large scale color--name pairs obtained from an online color design forum, we evaluate our model on a "color Turing test" and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at http://colorlab.us.</div>
                          <div id='notes16' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry17' class='bib entry'>

                    <a href='http://arxiv.org/abs/1502.03044v3' class='title'>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>
                    <br><span class='author'>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio.</span> <br>
                    <span class='year'>2015</span><br>
                    <div id='annotation17hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno17' class='bib annotation'>
                          <div id='abstract17' class='bib abstract'><b>Abstract</b>
                            Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.</div>
                          <div id='notes17' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry18' class='bib entry'>

                    <a href='http://arxiv.org/abs/1511.00060v3' class='title'>Top-down Tree Long Short-Term Memory Networks</a>
                    <br><span class='author'>Xingxing Zhang, Liang Lu, Mirella Lapata.</span> <br>
                    <span class='year'>2015</span><br>
                    <div id='annotation18hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno18' class='bib annotation'>
                          <div id='abstract18' class='bib abstract'><b>Abstract</b>
                            Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.</div>
                          <div id='notes18' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry19' class='bib entry'>

                    <a href='http://arxiv.org/abs/1603.06059v3' class='title'>Generating Natural Questions About an Image</a>
                    <br><span class='author'>Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, Lucy Vanderwende.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation19hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno19' class='bib annotation'>
                          <div id='abstract19' class='bib abstract'><b>Abstract</b>
                            There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language.</div>
                          <div id='notes19' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry20' class='bib entry'>

                    <a href='http://arxiv.org/abs/1604.03968v1' class='title'>Visual Storytelling</a>
                    <br><span class='author'>Ting-Hao,  Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation20hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno20' class='bib annotation'>
                          <div id='abstract20' class='bib abstract'><b>Abstract</b>
                            We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.</div>
                          <div id='notes20' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry21' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07710v1' class='title'>EmojiNet: Building a Machine Readable Sense Inventory for Emoji</a>
                    <br><span class='author'>Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation21hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno21' class='bib annotation'>
                          <div id='abstract21' class='bib abstract'><b>Abstract</b>
                            Emoji are a contemporary and extremely popular way to enhance electronic communication. Without rigid semantics attached to them, emoji symbols take on different meanings based on the context of a message. Thus, like the word sense disambiguation task in natural language processing, machines also need to disambiguate the meaning or sense of an emoji. In a first step toward achieving this goal, this paper presents EmojiNet, the first machine readable sense inventory for emoji. EmojiNet is a resource enabling systems to link emoji with their context-specific meaning. It is automatically constructed by integrating multiple emoji resources with BabelNet, which is the most comprehensive multilingual sense inventory available to date. The paper discusses its construction, evaluates the automatic resource creation process, and presents a use case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is available online for use at http://emojinet.knoesis.org.</div>
                          <div id='notes21' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry22' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07647v1' class='title'>Learning to Reason With Adaptive Computation</a>
                    <br><span class='author'>Mark Neumann, Pontus Stenetorp, Sebastian Riedel.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation22hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno22' class='bib annotation'>
                          <div id='abstract22' class='bib abstract'><b>Abstract</b>
                            Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model.</div>
                          <div id='notes22' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry23' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07708v1' class='title'>Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples</a>
                    <br><span class='author'>Amit Sheth, Sujan Perera, Sanjaya Wijeratne.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation23hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno23' class='bib annotation'>
                          <div id='abstract23' class='bib abstract'><b>Abstract</b>
                            Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data.</div>
                          <div id='notes23' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry24' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07918v1' class='title'>Sequence Segmentation Using Joint RNN and Structured Prediction Models</a>
                    <br><span class='author'>Yossi Adi, Joseph Keshet, Emily Cibelli, Matthew Goldrick.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation24hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno24' class='bib annotation'>
                          <div id='abstract24' class='bib abstract'><b>Abstract</b>
                            We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results sug- gest the proposed model is superior to previous methods, ob- taining state-of-the-art results on the tested datasets.</div>
                          <div id='notes24' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry25' class='bib entry'>

                    <a href='http://arxiv.org/abs/1604.04835v2' class='title'>SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions</a>
                    <br><span class='author'>Han Xiao, Minlie Huang, Xiaoyan Zhu.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation25hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno25' class='bib annotation'>
                          <div id='abstract25' class='bib abstract'><b>Abstract</b>
                            Knowledge representation is an important, long-history topic in AI, and there have been a large amount of work for knowledge graph embedding which projects symbolic entities and relations into low-dimensional, real-valued vector space. However, most embedding methods merely concentrate on data fitting and ignore the explicit semantic expression, leading to uninterpretable representations. Thus, traditional embedding methods have limited potentials for many applications such as question answering, and entity classification. To this end, this paper proposes a semantic representation method for knowledge graph \textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines substantially.</div>
                          <div id='notes25' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry26' class='bib entry'>

                    <a href='http://arxiv.org/abs/1604.06045v7' class='title'>Dialog-based Language Learning</a>
                    <br><span class='author'>Jason Weston.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation26hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno26' class='bib annotation'>
                          <div id='abstract26' class='bib abstract'><b>Abstract</b>
                            A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.</div>
                          <div id='notes26' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry27' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07149v1' class='title'>Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems</a>
                    <br><span class='author'>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, Ming Zhang.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation27hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno27' class='bib annotation'>
                          <div id='abstract27' class='bib abstract'><b>Abstract</b>
                            Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.</div>
                          <div id='notes27' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry28' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07365v1' class='title'>Introduction: Cognitive Issues in Natural Language Processing</a>
                    <br><span class='author'>Thierry Poibeau, Shravan Vasishth.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation28hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno28' class='bib annotation'>
                          <div id='abstract28' class='bib abstract'><b>Abstract</b>
                            This special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science. It specifically raises two questions: "what is the potential contribution of computational language modeling to cognitive science?" and conversely: "what is the influence of cognitive science in contemporary computational linguistics?"</div>
                          <div id='notes28' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry29' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07432v1' class='title'>Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research</a>
                    <br><span class='author'>Douwe Kiela, Luana Bulat, Anita L. Vero, Stephen Clark.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation29hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno29' class='bib annotation'>
                          <div id='abstract29' class='bib abstract'><b>Abstract</b>
                            Meaning has been called the "holy grail" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.</div>
                          <div id='notes29' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry30' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.07569v1' class='title'>Geometry of Polysemy</a>
                    <br><span class='author'>Jiaqi Mu, Suma Bhat, Pramod Viswanath.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation30hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno30' class='bib annotation'>
                          <div id='abstract30' class='bib abstract'><b>Abstract</b>
                            Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.</div>
                          <div id='notes30' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry31' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.06454v1' class='title'>Reasoning with Memory Augmented Neural Networks for Language Comprehension</a>
                    <br><span class='author'>Tsendsuren Munkhdalai, Hong Yu.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation31hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno31' class='bib annotation'>
                          <div id='abstract31' class='bib abstract'><b>Abstract</b>
                            Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.</div>
                          <div id='notes31' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry32' class='bib entry'>

                    <a href='http://arxiv.org/abs/1606.03821v2' class='title'>Learning to Generate Compositional Color Descriptions</a>
                    <br><span class='author'>Will Monroe, Noah D. Goodman, Christopher Potts.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation32hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno32' class='bib annotation'>
                          <div id='abstract32' class='bib abstract'><b>Abstract</b>
                            The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations ("greenish"), bare modifiers ("bright", "dull"), and compositional phrases ("faded teal") not seen in training.</div>
                          <div id='notes32' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry33' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.03035v2' class='title'>Latent Sequence Decompositions</a>
                    <br><span class='author'>William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation33hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno33' class='bib annotation'>
                          <div id='abstract33' class='bib abstract'><b>Abstract</b>
                            We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.2% WER.</div>
                          <div id='notes33' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry34' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.02906v3' class='title'>A General Framework for Content-enhanced Network Representation Learning</a>
                    <br><span class='author'>Xiaofei Sun, Jiang Guo, Xiao Ding, Ting Liu.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation34hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno34' class='bib annotation'>
                          <div id='abstract34' class='bib abstract'><b>Abstract</b>
                            This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.</div>
                          <div id='notes34' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry35' class='bib entry'>

                    <a href='http://arxiv.org/abs/1610.04211v1' class='title'>Gated End-to-End Memory Networks</a>
                    <br><span class='author'>Julien Perez, Fei Liu.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation35hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno35' class='bib annotation'>
                          <div id='abstract35' class='bib abstract'><b>Abstract</b>
                            Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.</div>
                          <div id='notes35' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry36' class='bib entry'>

                    <a href='http://arxiv.org/abs/1606.04052v4' class='title'>Dialog state tracking, a machine reading approach using Memory Network</a>
                    <br><span class='author'>Julien Perez, Fei Liu.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation36hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno36' class='bib annotation'>
                          <div id='abstract36' class='bib abstract'><b>Abstract</b>
                            In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model.</div>
                          <div id='notes36' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry37' class='bib entry'>

                    <a href='http://arxiv.org/abs/1607.01426v2' class='title'>Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks</a>
                    <br><span class='author'>Rajarshi Das, Arvind Neelakantan, David Belanger, Andrew McCallum.</span> <br>
                    <span class='year'>2016</span><br>
                    <div id='annotation37hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno37' class='bib annotation'>
                          <div id='abstract37' class='bib abstract'><b>Abstract</b>
                            Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art. The code and data are available at https://rajarshd.github.io/ChainsofReasoning</div>
                          <div id='notes37' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
</ul></div>


<script>var options={valueNames:['title','author','year', 'abstract', 'notes']};
var userList = new List('bibitems', options);</script><script>
        var bibentry0on = true;
        $('#annotation0hider').click(function() {
            console.trace('here');
            if (bibentry0on) {
                $('#annotation0hider').html('show annotations');
                $('#anno0.bib.annotation').css('display', 'none');
                bibentry0on = false;
            }
            else {
                $('#anno0.bib.annotation').css('display', 'inherit');
                $('#annotation0hider').html("hide annotations");
                bibentry0on = true;
            }
        });
        $('#annotation0hider').click();</script>
<script>
        var bibentry1on = true;
        $('#annotation1hider').click(function() {
            console.trace('here');
            if (bibentry1on) {
                $('#annotation1hider').html('show annotations');
                $('#anno1.bib.annotation').css('display', 'none');
                bibentry1on = false;
            }
            else {
                $('#anno1.bib.annotation').css('display', 'inherit');
                $('#annotation1hider').html("hide annotations");
                bibentry1on = true;
            }
        });
        $('#annotation1hider').click();</script>
<script>
        var bibentry2on = true;
        $('#annotation2hider').click(function() {
            console.trace('here');
            if (bibentry2on) {
                $('#annotation2hider').html('show annotations');
                $('#anno2.bib.annotation').css('display', 'none');
                bibentry2on = false;
            }
            else {
                $('#anno2.bib.annotation').css('display', 'inherit');
                $('#annotation2hider').html("hide annotations");
                bibentry2on = true;
            }
        });
        $('#annotation2hider').click();</script>
<script>
        var bibentry3on = true;
        $('#annotation3hider').click(function() {
            console.trace('here');
            if (bibentry3on) {
                $('#annotation3hider').html('show annotations');
                $('#anno3.bib.annotation').css('display', 'none');
                bibentry3on = false;
            }
            else {
                $('#anno3.bib.annotation').css('display', 'inherit');
                $('#annotation3hider').html("hide annotations");
                bibentry3on = true;
            }
        });
        $('#annotation3hider').click();</script>
<script>
        var bibentry4on = true;
        $('#annotation4hider').click(function() {
            console.trace('here');
            if (bibentry4on) {
                $('#annotation4hider').html('show annotations');
                $('#anno4.bib.annotation').css('display', 'none');
                bibentry4on = false;
            }
            else {
                $('#anno4.bib.annotation').css('display', 'inherit');
                $('#annotation4hider').html("hide annotations");
                bibentry4on = true;
            }
        });
        $('#annotation4hider').click();</script>
<script>
        var bibentry5on = true;
        $('#annotation5hider').click(function() {
            console.trace('here');
            if (bibentry5on) {
                $('#annotation5hider').html('show annotations');
                $('#anno5.bib.annotation').css('display', 'none');
                bibentry5on = false;
            }
            else {
                $('#anno5.bib.annotation').css('display', 'inherit');
                $('#annotation5hider').html("hide annotations");
                bibentry5on = true;
            }
        });
        $('#annotation5hider').click();</script>
<script>
        var bibentry6on = true;
        $('#annotation6hider').click(function() {
            console.trace('here');
            if (bibentry6on) {
                $('#annotation6hider').html('show annotations');
                $('#anno6.bib.annotation').css('display', 'none');
                bibentry6on = false;
            }
            else {
                $('#anno6.bib.annotation').css('display', 'inherit');
                $('#annotation6hider').html("hide annotations");
                bibentry6on = true;
            }
        });
        $('#annotation6hider').click();</script>
<script>
        var bibentry7on = true;
        $('#annotation7hider').click(function() {
            console.trace('here');
            if (bibentry7on) {
                $('#annotation7hider').html('show annotations');
                $('#anno7.bib.annotation').css('display', 'none');
                bibentry7on = false;
            }
            else {
                $('#anno7.bib.annotation').css('display', 'inherit');
                $('#annotation7hider').html("hide annotations");
                bibentry7on = true;
            }
        });
        $('#annotation7hider').click();</script>
<script>
        var bibentry8on = true;
        $('#annotation8hider').click(function() {
            console.trace('here');
            if (bibentry8on) {
                $('#annotation8hider').html('show annotations');
                $('#anno8.bib.annotation').css('display', 'none');
                bibentry8on = false;
            }
            else {
                $('#anno8.bib.annotation').css('display', 'inherit');
                $('#annotation8hider').html("hide annotations");
                bibentry8on = true;
            }
        });
        $('#annotation8hider').click();</script>
<script>
        var bibentry9on = true;
        $('#annotation9hider').click(function() {
            console.trace('here');
            if (bibentry9on) {
                $('#annotation9hider').html('show annotations');
                $('#anno9.bib.annotation').css('display', 'none');
                bibentry9on = false;
            }
            else {
                $('#anno9.bib.annotation').css('display', 'inherit');
                $('#annotation9hider').html("hide annotations");
                bibentry9on = true;
            }
        });
        $('#annotation9hider').click();</script>
<script>
        var bibentry10on = true;
        $('#annotation10hider').click(function() {
            console.trace('here');
            if (bibentry10on) {
                $('#annotation10hider').html('show annotations');
                $('#anno10.bib.annotation').css('display', 'none');
                bibentry10on = false;
            }
            else {
                $('#anno10.bib.annotation').css('display', 'inherit');
                $('#annotation10hider').html("hide annotations");
                bibentry10on = true;
            }
        });
        $('#annotation10hider').click();</script>
<script>
        var bibentry11on = true;
        $('#annotation11hider').click(function() {
            console.trace('here');
            if (bibentry11on) {
                $('#annotation11hider').html('show annotations');
                $('#anno11.bib.annotation').css('display', 'none');
                bibentry11on = false;
            }
            else {
                $('#anno11.bib.annotation').css('display', 'inherit');
                $('#annotation11hider').html("hide annotations");
                bibentry11on = true;
            }
        });
        $('#annotation11hider').click();</script>
<script>
        var bibentry12on = true;
        $('#annotation12hider').click(function() {
            console.trace('here');
            if (bibentry12on) {
                $('#annotation12hider').html('show annotations');
                $('#anno12.bib.annotation').css('display', 'none');
                bibentry12on = false;
            }
            else {
                $('#anno12.bib.annotation').css('display', 'inherit');
                $('#annotation12hider').html("hide annotations");
                bibentry12on = true;
            }
        });
        $('#annotation12hider').click();</script>
<script>
        var bibentry13on = true;
        $('#annotation13hider').click(function() {
            console.trace('here');
            if (bibentry13on) {
                $('#annotation13hider').html('show annotations');
                $('#anno13.bib.annotation').css('display', 'none');
                bibentry13on = false;
            }
            else {
                $('#anno13.bib.annotation').css('display', 'inherit');
                $('#annotation13hider').html("hide annotations");
                bibentry13on = true;
            }
        });
        $('#annotation13hider').click();</script>
<script>
        var bibentry14on = true;
        $('#annotation14hider').click(function() {
            console.trace('here');
            if (bibentry14on) {
                $('#annotation14hider').html('show annotations');
                $('#anno14.bib.annotation').css('display', 'none');
                bibentry14on = false;
            }
            else {
                $('#anno14.bib.annotation').css('display', 'inherit');
                $('#annotation14hider').html("hide annotations");
                bibentry14on = true;
            }
        });
        $('#annotation14hider').click();</script>
<script>
        var bibentry15on = true;
        $('#annotation15hider').click(function() {
            console.trace('here');
            if (bibentry15on) {
                $('#annotation15hider').html('show annotations');
                $('#anno15.bib.annotation').css('display', 'none');
                bibentry15on = false;
            }
            else {
                $('#anno15.bib.annotation').css('display', 'inherit');
                $('#annotation15hider').html("hide annotations");
                bibentry15on = true;
            }
        });
        $('#annotation15hider').click();</script>
<script>
        var bibentry16on = true;
        $('#annotation16hider').click(function() {
            console.trace('here');
            if (bibentry16on) {
                $('#annotation16hider').html('show annotations');
                $('#anno16.bib.annotation').css('display', 'none');
                bibentry16on = false;
            }
            else {
                $('#anno16.bib.annotation').css('display', 'inherit');
                $('#annotation16hider').html("hide annotations");
                bibentry16on = true;
            }
        });
        $('#annotation16hider').click();</script>
<script>
        var bibentry17on = true;
        $('#annotation17hider').click(function() {
            console.trace('here');
            if (bibentry17on) {
                $('#annotation17hider').html('show annotations');
                $('#anno17.bib.annotation').css('display', 'none');
                bibentry17on = false;
            }
            else {
                $('#anno17.bib.annotation').css('display', 'inherit');
                $('#annotation17hider').html("hide annotations");
                bibentry17on = true;
            }
        });
        $('#annotation17hider').click();</script>
<script>
        var bibentry18on = true;
        $('#annotation18hider').click(function() {
            console.trace('here');
            if (bibentry18on) {
                $('#annotation18hider').html('show annotations');
                $('#anno18.bib.annotation').css('display', 'none');
                bibentry18on = false;
            }
            else {
                $('#anno18.bib.annotation').css('display', 'inherit');
                $('#annotation18hider').html("hide annotations");
                bibentry18on = true;
            }
        });
        $('#annotation18hider').click();</script>
<script>
        var bibentry19on = true;
        $('#annotation19hider').click(function() {
            console.trace('here');
            if (bibentry19on) {
                $('#annotation19hider').html('show annotations');
                $('#anno19.bib.annotation').css('display', 'none');
                bibentry19on = false;
            }
            else {
                $('#anno19.bib.annotation').css('display', 'inherit');
                $('#annotation19hider').html("hide annotations");
                bibentry19on = true;
            }
        });
        $('#annotation19hider').click();</script>
<script>
        var bibentry20on = true;
        $('#annotation20hider').click(function() {
            console.trace('here');
            if (bibentry20on) {
                $('#annotation20hider').html('show annotations');
                $('#anno20.bib.annotation').css('display', 'none');
                bibentry20on = false;
            }
            else {
                $('#anno20.bib.annotation').css('display', 'inherit');
                $('#annotation20hider').html("hide annotations");
                bibentry20on = true;
            }
        });
        $('#annotation20hider').click();</script>
<script>
        var bibentry21on = true;
        $('#annotation21hider').click(function() {
            console.trace('here');
            if (bibentry21on) {
                $('#annotation21hider').html('show annotations');
                $('#anno21.bib.annotation').css('display', 'none');
                bibentry21on = false;
            }
            else {
                $('#anno21.bib.annotation').css('display', 'inherit');
                $('#annotation21hider').html("hide annotations");
                bibentry21on = true;
            }
        });
        $('#annotation21hider').click();</script>
<script>
        var bibentry22on = true;
        $('#annotation22hider').click(function() {
            console.trace('here');
            if (bibentry22on) {
                $('#annotation22hider').html('show annotations');
                $('#anno22.bib.annotation').css('display', 'none');
                bibentry22on = false;
            }
            else {
                $('#anno22.bib.annotation').css('display', 'inherit');
                $('#annotation22hider').html("hide annotations");
                bibentry22on = true;
            }
        });
        $('#annotation22hider').click();</script>
<script>
        var bibentry23on = true;
        $('#annotation23hider').click(function() {
            console.trace('here');
            if (bibentry23on) {
                $('#annotation23hider').html('show annotations');
                $('#anno23.bib.annotation').css('display', 'none');
                bibentry23on = false;
            }
            else {
                $('#anno23.bib.annotation').css('display', 'inherit');
                $('#annotation23hider').html("hide annotations");
                bibentry23on = true;
            }
        });
        $('#annotation23hider').click();</script>
<script>
        var bibentry24on = true;
        $('#annotation24hider').click(function() {
            console.trace('here');
            if (bibentry24on) {
                $('#annotation24hider').html('show annotations');
                $('#anno24.bib.annotation').css('display', 'none');
                bibentry24on = false;
            }
            else {
                $('#anno24.bib.annotation').css('display', 'inherit');
                $('#annotation24hider').html("hide annotations");
                bibentry24on = true;
            }
        });
        $('#annotation24hider').click();</script>
<script>
        var bibentry25on = true;
        $('#annotation25hider').click(function() {
            console.trace('here');
            if (bibentry25on) {
                $('#annotation25hider').html('show annotations');
                $('#anno25.bib.annotation').css('display', 'none');
                bibentry25on = false;
            }
            else {
                $('#anno25.bib.annotation').css('display', 'inherit');
                $('#annotation25hider').html("hide annotations");
                bibentry25on = true;
            }
        });
        $('#annotation25hider').click();</script>
<script>
        var bibentry26on = true;
        $('#annotation26hider').click(function() {
            console.trace('here');
            if (bibentry26on) {
                $('#annotation26hider').html('show annotations');
                $('#anno26.bib.annotation').css('display', 'none');
                bibentry26on = false;
            }
            else {
                $('#anno26.bib.annotation').css('display', 'inherit');
                $('#annotation26hider').html("hide annotations");
                bibentry26on = true;
            }
        });
        $('#annotation26hider').click();</script>
<script>
        var bibentry27on = true;
        $('#annotation27hider').click(function() {
            console.trace('here');
            if (bibentry27on) {
                $('#annotation27hider').html('show annotations');
                $('#anno27.bib.annotation').css('display', 'none');
                bibentry27on = false;
            }
            else {
                $('#anno27.bib.annotation').css('display', 'inherit');
                $('#annotation27hider').html("hide annotations");
                bibentry27on = true;
            }
        });
        $('#annotation27hider').click();</script>
<script>
        var bibentry28on = true;
        $('#annotation28hider').click(function() {
            console.trace('here');
            if (bibentry28on) {
                $('#annotation28hider').html('show annotations');
                $('#anno28.bib.annotation').css('display', 'none');
                bibentry28on = false;
            }
            else {
                $('#anno28.bib.annotation').css('display', 'inherit');
                $('#annotation28hider').html("hide annotations");
                bibentry28on = true;
            }
        });
        $('#annotation28hider').click();</script>
<script>
        var bibentry29on = true;
        $('#annotation29hider').click(function() {
            console.trace('here');
            if (bibentry29on) {
                $('#annotation29hider').html('show annotations');
                $('#anno29.bib.annotation').css('display', 'none');
                bibentry29on = false;
            }
            else {
                $('#anno29.bib.annotation').css('display', 'inherit');
                $('#annotation29hider').html("hide annotations");
                bibentry29on = true;
            }
        });
        $('#annotation29hider').click();</script>
<script>
        var bibentry30on = true;
        $('#annotation30hider').click(function() {
            console.trace('here');
            if (bibentry30on) {
                $('#annotation30hider').html('show annotations');
                $('#anno30.bib.annotation').css('display', 'none');
                bibentry30on = false;
            }
            else {
                $('#anno30.bib.annotation').css('display', 'inherit');
                $('#annotation30hider').html("hide annotations");
                bibentry30on = true;
            }
        });
        $('#annotation30hider').click();</script>
<script>
        var bibentry31on = true;
        $('#annotation31hider').click(function() {
            console.trace('here');
            if (bibentry31on) {
                $('#annotation31hider').html('show annotations');
                $('#anno31.bib.annotation').css('display', 'none');
                bibentry31on = false;
            }
            else {
                $('#anno31.bib.annotation').css('display', 'inherit');
                $('#annotation31hider').html("hide annotations");
                bibentry31on = true;
            }
        });
        $('#annotation31hider').click();</script>
<script>
        var bibentry32on = true;
        $('#annotation32hider').click(function() {
            console.trace('here');
            if (bibentry32on) {
                $('#annotation32hider').html('show annotations');
                $('#anno32.bib.annotation').css('display', 'none');
                bibentry32on = false;
            }
            else {
                $('#anno32.bib.annotation').css('display', 'inherit');
                $('#annotation32hider').html("hide annotations");
                bibentry32on = true;
            }
        });
        $('#annotation32hider').click();</script>
<script>
        var bibentry33on = true;
        $('#annotation33hider').click(function() {
            console.trace('here');
            if (bibentry33on) {
                $('#annotation33hider').html('show annotations');
                $('#anno33.bib.annotation').css('display', 'none');
                bibentry33on = false;
            }
            else {
                $('#anno33.bib.annotation').css('display', 'inherit');
                $('#annotation33hider').html("hide annotations");
                bibentry33on = true;
            }
        });
        $('#annotation33hider').click();</script>
<script>
        var bibentry34on = true;
        $('#annotation34hider').click(function() {
            console.trace('here');
            if (bibentry34on) {
                $('#annotation34hider').html('show annotations');
                $('#anno34.bib.annotation').css('display', 'none');
                bibentry34on = false;
            }
            else {
                $('#anno34.bib.annotation').css('display', 'inherit');
                $('#annotation34hider').html("hide annotations");
                bibentry34on = true;
            }
        });
        $('#annotation34hider').click();</script>
<script>
        var bibentry35on = true;
        $('#annotation35hider').click(function() {
            console.trace('here');
            if (bibentry35on) {
                $('#annotation35hider').html('show annotations');
                $('#anno35.bib.annotation').css('display', 'none');
                bibentry35on = false;
            }
            else {
                $('#anno35.bib.annotation').css('display', 'inherit');
                $('#annotation35hider').html("hide annotations");
                bibentry35on = true;
            }
        });
        $('#annotation35hider').click();</script>
<script>
        var bibentry36on = true;
        $('#annotation36hider').click(function() {
            console.trace('here');
            if (bibentry36on) {
                $('#annotation36hider').html('show annotations');
                $('#anno36.bib.annotation').css('display', 'none');
                bibentry36on = false;
            }
            else {
                $('#anno36.bib.annotation').css('display', 'inherit');
                $('#annotation36hider').html("hide annotations");
                bibentry36on = true;
            }
        });
        $('#annotation36hider').click();</script>
<script>
        var bibentry37on = true;
        $('#annotation37hider').click(function() {
            console.trace('here');
            if (bibentry37on) {
                $('#annotation37hider').html('show annotations');
                $('#anno37.bib.annotation').css('display', 'none');
                bibentry37on = false;
            }
            else {
                $('#anno37.bib.annotation').css('display', 'inherit');
                $('#annotation37hider').html("hide annotations");
                bibentry37on = true;
            }
        });
        $('#annotation37hider').click();</script>
<script> var allshowing=false; 
function hideall() { 
allshowing=false;bibentry0on = true; $('#annotation0hider').click();
bibentry1on = true; $('#annotation1hider').click();
bibentry2on = true; $('#annotation2hider').click();
bibentry3on = true; $('#annotation3hider').click();
bibentry4on = true; $('#annotation4hider').click();
bibentry5on = true; $('#annotation5hider').click();
bibentry6on = true; $('#annotation6hider').click();
bibentry7on = true; $('#annotation7hider').click();
bibentry8on = true; $('#annotation8hider').click();
bibentry9on = true; $('#annotation9hider').click();
bibentry10on = true; $('#annotation10hider').click();
bibentry11on = true; $('#annotation11hider').click();
bibentry12on = true; $('#annotation12hider').click();
bibentry13on = true; $('#annotation13hider').click();
bibentry14on = true; $('#annotation14hider').click();
bibentry15on = true; $('#annotation15hider').click();
bibentry16on = true; $('#annotation16hider').click();
bibentry17on = true; $('#annotation17hider').click();
bibentry18on = true; $('#annotation18hider').click();
bibentry19on = true; $('#annotation19hider').click();
bibentry20on = true; $('#annotation20hider').click();
bibentry21on = true; $('#annotation21hider').click();
bibentry22on = true; $('#annotation22hider').click();
bibentry23on = true; $('#annotation23hider').click();
bibentry24on = true; $('#annotation24hider').click();
bibentry25on = true; $('#annotation25hider').click();
bibentry26on = true; $('#annotation26hider').click();
bibentry27on = true; $('#annotation27hider').click();
bibentry28on = true; $('#annotation28hider').click();
bibentry29on = true; $('#annotation29hider').click();
bibentry30on = true; $('#annotation30hider').click();
bibentry31on = true; $('#annotation31hider').click();
bibentry32on = true; $('#annotation32hider').click();
bibentry33on = true; $('#annotation33hider').click();
bibentry34on = true; $('#annotation34hider').click();
bibentry35on = true; $('#annotation35hider').click();
bibentry36on = true; $('#annotation36hider').click();
bibentry37on = true; $('#annotation37hider').click();
}
function showall() { 
allshowing=true;
bibentry0on = false; $('#annotation0hider').click();
bibentry1on = false; $('#annotation1hider').click();
bibentry2on = false; $('#annotation2hider').click();
bibentry3on = false; $('#annotation3hider').click();
bibentry4on = false; $('#annotation4hider').click();
bibentry5on = false; $('#annotation5hider').click();
bibentry6on = false; $('#annotation6hider').click();
bibentry7on = false; $('#annotation7hider').click();
bibentry8on = false; $('#annotation8hider').click();
bibentry9on = false; $('#annotation9hider').click();
bibentry10on = false; $('#annotation10hider').click();
bibentry11on = false; $('#annotation11hider').click();
bibentry12on = false; $('#annotation12hider').click();
bibentry13on = false; $('#annotation13hider').click();
bibentry14on = false; $('#annotation14hider').click();
bibentry15on = false; $('#annotation15hider').click();
bibentry16on = false; $('#annotation16hider').click();
bibentry17on = false; $('#annotation17hider').click();
bibentry18on = false; $('#annotation18hider').click();
bibentry19on = false; $('#annotation19hider').click();
bibentry20on = false; $('#annotation20hider').click();
bibentry21on = false; $('#annotation21hider').click();
bibentry22on = false; $('#annotation22hider').click();
bibentry23on = false; $('#annotation23hider').click();
bibentry24on = false; $('#annotation24hider').click();
bibentry25on = false; $('#annotation25hider').click();
bibentry26on = false; $('#annotation26hider').click();
bibentry27on = false; $('#annotation27hider').click();
bibentry28on = false; $('#annotation28hider').click();
bibentry29on = false; $('#annotation29hider').click();
bibentry30on = false; $('#annotation30hider').click();
bibentry31on = false; $('#annotation31hider').click();
bibentry32on = false; $('#annotation32hider').click();
bibentry33on = false; $('#annotation33hider').click();
bibentry34on = false; $('#annotation34hider').click();
bibentry35on = false; $('#annotation35hider').click();
bibentry36on = false; $('#annotation36hider').click();
bibentry37on = false; $('#annotation37hider').click();
}
$('#showhide').click(function() {if (allshowing) { hideall(); $('#showhide').html('Expand all annoations');}else { showall(); $('#showhide').html('Hide all annoations');}});</script>
