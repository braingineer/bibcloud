<blockquote><p>papers for inverse and standard reinforcement learning</p></blockquote>

<div id='bibitems'><div class='btn-container'><br><input class='search' placeholder='Search title, authors, abstract, and notes'/><br><button class='sort' data-sort='author'>Sort by Author</button><button class='sort' data-sort='year'>Sort by Year</button><button class='sort' data-sort='title'>Sort by Title</button><button id='showhide'>Expand all annotations</button></div>
<ul class='bib list'><li>
                  <div id='entry0' class='bib entry'>

                    <a href='http://jmlr.csail.mit.edu/proceedings/papers/v15/boularias11a/boularias11a.pdf' class='title'>Relative entropy inverse reinforcement learning</a>
                    <br><span class='author'>Boularias, Abdeslam and Kober, Jens and Peters, Jan R.</span> <br>
                    <span class='year'>2011</span><br>
                    <div id='annotation0hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno0' class='bib annotation'>
                          <div id='abstract0' class='bib abstract'><b>Abstract</b>
                            We consider the problem of imitation learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)- optimal policy can be computed for different reward functions. However, this requirement can hardly be satisfied in systems with a large, or continuous, state space. In this paper, we propose a model-free IRL algorithm, where the relative entropy between the empirical distribution of the state-action trajectories under a baseline policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using learned MDP models. Empirical results on simulated car racing, gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.</div>
                          <div id='notes0' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry1' class='bib entry'>

                    <a href='https://www.cs.uic.edu/pub/Ziebart/Publications/maxentirl-bziebart.pdf' class='title'>Maximum Entropy Inverse Reinforcement Learning.</a>
                    <br><span class='author'>Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K.</span> <br>
                    <span class='year'>2008</span><br>
                    <div id='annotation1hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno1' class='bib annotation'>
                          <div id='abstract1' class='bib abstract'><b>Abstract</b>
                            Empty</div>
                          <div id='notes1' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry2' class='bib entry'>

                    <a href='http://jmlr.csail.mit.edu/proceedings/papers/v24/heess12a/heess12a.pdf' class='title'>Actor-Critic Reinforcement Learning with Energy-Based Policies.</a>
                    <br><span class='author'>Heess, Nicolas and Silver, David and Teh, Yee Whye.</span> <br>
                    <span class='year'>2012</span><br>
                    <div id='annotation2hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno2' class='bib annotation'>
                          <div id='abstract2' class='bib abstract'><b>Abstract</b>
                            We consider reinforcement learning in Markov decision processes with high dimensional state and action spaces. We parametrize policies using energy-based models (particularly restricted Boltzmann machines), and train them using policy gradient learning. Our approach builds upon Sallans and Hinton (2004), who parameterized value functions using energy-based models, trained using a non-linear variant of temporal-difference (TD) learning. Unfortunately, non-linear TD is known to diverge in theory and practice. We introduce the first sound and efficient algorithm for training energy-based policies, based on an actorcritic architecture. Our algorithm is computationally efficient, converges close to a local optimum, and outperforms Sallans and Hinton (2004) in several high dimensional domains.</div>
                          <div id='notes2' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry3' class='bib entry'>

                    <a href='http://papers.nips.cc/paper/3258-incremental-natural-actor-critic-algorithms.pdf' class='title'>Incremental natural actor-critic algorithms</a>
                    <br><span class='author'>Bhatnagar, Shalabh and Ghavamzadeh, Mohammad and Lee, Mark and Sutton, Richard S.</span> <br>
                    <span class='year'>2007</span><br>
                    <div id='annotation3hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno3' class='bib annotation'>
                          <div id='abstract3' class='bib abstract'><b>Abstract</b>
                            Empty</div>
                          <div id='notes3' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry4' class='bib entry'>

                    <a href='http://www.cs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf' class='title'>Inverse Reinforcement Learnign class slides</a>
                    <br><span class='author'>Pieter Abbeel.</span> <br>
                    <span class='year'>2012</span><br>
                    <div id='annotation4hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno4' class='bib annotation'>
                          <div id='abstract4' class='bib abstract'><b>Abstract</b>
                            </div>
                          <div id='notes4' class='bib notes'><b>Notes</b>
                           some good references and notes for IRL stuff</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry5' class='bib entry'>

                    <a href='http://papers.nips.cc/paper/4551-inverse-reinforcement-learning-through-structured-classification.pdf' class='title'>Inverse reinforcement learning through structured classification</a>
                    <br><span class='author'>Klein, Edouard and Geist, Matthieu and Piot, Bilal and Pietquin, Olivier.</span> <br>
                    <span class='year'>2012</span><br>
                    <div id='annotation5hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno5' class='bib annotation'>
                          <div id='abstract5' class='bib abstract'><b>Abstract</b>
                            This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving the direct RL problem. Moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator.</div>
                          <div id='notes5' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry6' class='bib entry'>

                    <a href='http://www.jmlr.org/papers/volume12/choi11a/choi11a.pdf' class='title'>Inverse reinforcement learning in partially observable environments</a>
                    <br><span class='author'>Choi, Jaedeug and Kim, Kee-Eung.</span> <br>
                    <span class='year'>2011</span><br>
                    <div id='annotation6hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno6' class='bib annotation'>
                          <div id='abstract6' class='bib abstract'><b>Abstract</b>
                            Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert’s behavior, namely the case in which the expert’s policy is explicitly given, and the case in which the expert’s trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings</div>
                          <div id='notes6' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry7' class='bib entry'>

                    <a href='#' class='title'>Algorithms for inverse reinforcement learning.</a>
                    <br><span class='author'>Ng, Andrew Y and Russell, Stuart J and others.</span> <br>
                    <span class='year'>2000</span><br>
                    <div id='annotation7hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno7' class='bib annotation'>
                          <div id='abstract7' class='bib abstract'><b>Abstract</b>
                            This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is the problem of extracting a reward function given observed, optimal behavior.  IRL may be useful for apprenticeship learning to acquire skilled behavior, and for ascertaining the reward function being optimized by a natural system.  We first characterize the set of all reward functions for which a given policy is optimal.  We then derive three algorithms for IRL.  The first two deal with the case where the entire policy is known; we handle tabulated reward functions on a finite state space and linear functional approximation of the reward function over a potentially infinite state space.  The third algorithm deals with the more realistic case in which the policy is known only through a finite set of observed trajectories.  In all cases, a key issue is degeneracy---the existence of a large set of reward functions for which the observed policy is optimal.  To remove degeneracy, we suggest that some natural heuristics that attempt to pick a reward function that maximally differentiates the observed policy from other, sub-optimal policy.  This results in an efficiently solvable linear programming formulation of the IRL problem.  We demonstrate our algorithms on simple discrete/finite and continuous/infinite problems</div>
                          <div id='notes7' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry8' class='bib entry'>

                    <a href='https://www.youtube.com/watch?v=TAmORo9Kr1Y' class='title'>Lecture 7: Policy Gradient Methods (youtube)</a>
                    <br><span class='author'>David Silver.</span> <br>
                    <span class='year'>Empty</span><br>
                    <div id='annotation8hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno8' class='bib annotation'>
                          <div id='abstract8' class='bib abstract'><b>Abstract</b>
                            Empty</div>
                          <div id='notes8' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry9' class='bib entry'>

                    <a href='http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf' class='title'>Lecture 7: Policy Gradient Methods (slides)</a>
                    <br><span class='author'>David Silver.</span> <br>
                    <span class='year'>Empty</span><br>
                    <div id='annotation9hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno9' class='bib annotation'>
                          <div id='abstract9' class='bib abstract'><b>Abstract</b>
                            Empty</div>
                          <div id='notes9' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry10' class='bib entry'>

                    <a href='http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf' class='title'>Apprenticeship learning via inverse reinforcement learning</a>
                    <br><span class='author'>Abbeel, Pieter and Ng, Andrew Y.</span> <br>
                    <span class='year'>2004</span><br>
                    <div id='annotation10hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno10' class='bib annotation'>
                          <div id='abstract10' class='bib abstract'><b>Abstract</b>
                            We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using “inverse reinforcement learning” to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert’s reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert’s unknown reward function</div>
                          <div id='notes10' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry11' class='bib entry'>

                    <a href='http://reason.cs.uiuc.edu/deepak/irl.pdf' class='title'>Bayesian inverse reinforcement learning</a>
                    <br><span class='author'>Ramachandran, Deepak and Amir, Eyal.</span> <br>
                    <span class='year'>2007</span><br>
                    <div id='annotation11hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno11' class='bib annotation'>
                          <div id='abstract11' class='bib abstract'><b>Abstract</b>
                            Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches</div>
                          <div id='notes11' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry12' class='bib entry'>

                    <a href='http://arxiv.org/pdf/1202.1558v1.pdf' class='title'>On the Performance of Maximum Likelihood Inverse Reinforcement Learning</a>
                    <br><span class='author'>Ratia, H{\'e}ctor and Montesano, Luis and Martinez-Cantin, Ruben.</span> <br>
                    <span class='year'>2012</span><br>
                    <div id='annotation12hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno12' class='bib annotation'>
                          <div id='abstract12' class='bib abstract'><b>Abstract</b>
                            Inverse reinforcement learning (IRL) addresses the problem of recovering a task description given a demonstration of the optimal policy used to solve such a task. The optimal policy is usually provided by an expert or teacher, making IRL specially suitable for the problem of apprenticeship learning. The task description is encoded in the form of a reward function of a Markov decision process (MDP). Several algorithms have been proposed to find the reward function corresponding to a set of demonstrations. One of the algorithms that has provided best results in different applications is a gradient method to optimize a policy squared error criterion. On a parallel line of research, other authors have presented recently a gradient approximation of the maximum likelihood estimate of the reward signal. In general, both approaches approximate the gradient estimate and the criteria at different stages to make the algorithm tractable and efficient. In this work, we provide a detailed description of the different methods to highlight differences in terms of reward estimation, policy similarity and computational costs. We also provide experimental results to evaluate the differences in performance of the methods.</div>
                          <div id='notes12' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry13' class='bib entry'>

                    <a href='http://arxiv.org/pdf/1206.5264.pdf' class='title'>Apprenticeship learning using inverse reinforcement learning and gradient methods</a>
                    <br><span class='author'>Neu, Gergely and Szepesv{\'a}ri, Csaba.</span> <br>
                    <span class='year'>2012</span><br>
                    <div id='annotation13hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno13' class='bib annotation'>
                          <div id='abstract13' class='bib abstract'><b>Abstract</b>
                            In this paper we propose a novel gradient algorithm to learn a policy from an expert’s observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm’s aim is to find a reward function such that the resulting optimal policy matches well the expert’s observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is overcome by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.</div>
                          <div id='notes13' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry14' class='bib entry'>

                    <a href='#' class='title'>Knowledge and Ignorance in Reinforcement Learning</a>
                    <br><span class='author'>Deepak Ramachandran.</span> <br>
                    <span class='year'>2011</span><br>
                    <div id='annotation14hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno14' class='bib annotation'>
                          <div id='abstract14' class='bib abstract'><b>Abstract</b>
                            The field of Reinforcement Learning is concerned with teaching agents to take optimal decisions to maximize their total utility in complicated environments. A Reinforcement Learning problem, generally described by the Markov Decision Process formalism, has several complex interacting components, unlike in other machine learning settings. I distinguish three: the state-space/ transition model, the reward function, and the observation model. In this thesis, I present a framework for studying how the state of knowledge or uncertainty of each component affects the Reinforcement Learning process. I focus on the reward function and the observation model, which has traditionally received little attention. Algorithms for learning good policies when these components are completely specified are well understood. However, it is less clear what to do when they are unknown, uncertain or irrelevant. In this thesis, I describe how to adapt Reinforcement Learning algorithms to cope with these situations. Recently there has been great interest in the Inverse Reinforcement Learning problem where the objective is to learn the reward function from evidence of an agent’s reward-maximizing policy. The usual goal is to perform apprenticeship learning where the agent learns the optimal action to perform from an expert. However, sometimes the reward function is of independent interest as well. I describe a Bayesian Inverse Reinforcement Learning approach to this problem. BIRL uses a generative model to describe the decision-making process and by inverting it we can infer the reward function from action observations. It is distinguished from other IRL approaches by placing emphasis on the accuracy of the reward function in itself, and not just as an intermediate step to apprenticeship learning. BIRL is also able to handle incomplete and contradictory information from the expert. It has been applied successfully to preference elicitation problem for computer games and robot manipulation. In a recent comparison of IRL approaches, BIRL was the best-performing general IRL algorithm. I also extend this model to do a related task, Reward Shaping. In reward ii shaping, we seek to adjust a known reward function to make the learning agent converge on the optimal policy as fast as possible. Reward shaping has been proposed and studied previously in many applications, typically using additive potential functions. However the requirement of absolute policy-invariance is too strict to admit many useful cases of shaping. I define Bayesian Reward Shaping, which is a generalization to a soft form of reward shaping, and provide algorithms for achieving it. The impact of observation models on reinforcement learning has been studied even less than reward functions. This is surpising, considering how adding partial observability to an MDP model blows up the computational complexity and hence a better understanding of the tradeoffs between representational accuracy and ef- ficiency would be helpful. In this work, I describe how in certain cases POMDPs can be approximated by MDPs or slightly more complicated models with bounded performance loss. I also present an algorithm, called Smoothed Q-Learning for learning policies when the observation models are uncertain. Smoothed Sarsa is based on the idea that in many real-world POMDPs better state estimates can be made at later time steps and thus delaying the backup step of a temporal difference-based algorithm can shortcut the uncertainty in the obervation model and approximate the underlying MDP better. Combining these approaches together (Bayesian Reward Shaping and Smoothed Sarsa), a mobile robot was trained to execute delivery tasks in an office environment.</div>
                          <div id='notes14' class='bib notes'><b>Notes</b>
                           Empty</div>
                    </div>
                  </div>
                 </li>
                
<li>
                  <div id='entry15' class='bib entry'>

                    <a href='https://flowers.inria.fr/mlopes/myrefs/09-ecml-airl.pdf' class='title'>Active learning for reward estimation in inverse reinforcement learning</a>
                    <br><span class='author'>Lopes, Manuel and Melo, Francisco and Montesano, Luis.</span> <br>
                    <span class='year'>2009</span><br>
                    <div id='annotation15hider' class='bib no-selection btn'>hide annotations</div>
                    <div id='anno15' class='bib annotation'>
                          <div id='abstract15' class='bib abstract'><b>Abstract</b>
                            Inverse reinforcement learning addresses the general problem of recovering a reward function from samples of a policy provided by an expert/demonstrator.  In this paper, we introduce active learning for inverse reinforcement learning.  We propose an algorithm that allows the agent to query the demonstrator for samples at specific states, instead of relying only on samples provided at "arbitrary" states.  The purpose of our algorithm is to estimate the reward function with similar accuracy as other methods from the literature while reducing the amount of policy samples required from the expert.  We also discuss the use of our algorithm in higher dimensional problems, using both Monte Carlo and gradient methods.  We present illustrative results of our algorithm in several simulated examples of different complexities.</div>
                          <div id='notes15' class='bib notes'><b>Notes</b>
                           introduces MLIRL in this paper</div>
                    </div>
                  </div>
                 </li>
                
</ul></div>


<script>var options={valueNames:['title','author','year', 'abstract', 'notes']};
var userList = new List('bibitems', options);</script><script>
        var bibentry0on = true;
        $('#annotation0hider').click(function() {
            console.trace('here');
            if (bibentry0on) {
                $('#annotation0hider').html('show annotations');
                $('#anno0.bib.annotation').css('display', 'none');
                bibentry0on = false;
            }
            else {
                $('#anno0.bib.annotation').css('display', 'inherit');
                $('#annotation0hider').html("hide annotations");
                bibentry0on = true;
            }
        });
        $('#annotation0hider').click();</script>
<script>
        var bibentry1on = true;
        $('#annotation1hider').click(function() {
            console.trace('here');
            if (bibentry1on) {
                $('#annotation1hider').html('show annotations');
                $('#anno1.bib.annotation').css('display', 'none');
                bibentry1on = false;
            }
            else {
                $('#anno1.bib.annotation').css('display', 'inherit');
                $('#annotation1hider').html("hide annotations");
                bibentry1on = true;
            }
        });
        $('#annotation1hider').click();</script>
<script>
        var bibentry2on = true;
        $('#annotation2hider').click(function() {
            console.trace('here');
            if (bibentry2on) {
                $('#annotation2hider').html('show annotations');
                $('#anno2.bib.annotation').css('display', 'none');
                bibentry2on = false;
            }
            else {
                $('#anno2.bib.annotation').css('display', 'inherit');
                $('#annotation2hider').html("hide annotations");
                bibentry2on = true;
            }
        });
        $('#annotation2hider').click();</script>
<script>
        var bibentry3on = true;
        $('#annotation3hider').click(function() {
            console.trace('here');
            if (bibentry3on) {
                $('#annotation3hider').html('show annotations');
                $('#anno3.bib.annotation').css('display', 'none');
                bibentry3on = false;
            }
            else {
                $('#anno3.bib.annotation').css('display', 'inherit');
                $('#annotation3hider').html("hide annotations");
                bibentry3on = true;
            }
        });
        $('#annotation3hider').click();</script>
<script>
        var bibentry4on = true;
        $('#annotation4hider').click(function() {
            console.trace('here');
            if (bibentry4on) {
                $('#annotation4hider').html('show annotations');
                $('#anno4.bib.annotation').css('display', 'none');
                bibentry4on = false;
            }
            else {
                $('#anno4.bib.annotation').css('display', 'inherit');
                $('#annotation4hider').html("hide annotations");
                bibentry4on = true;
            }
        });
        $('#annotation4hider').click();</script>
<script>
        var bibentry5on = true;
        $('#annotation5hider').click(function() {
            console.trace('here');
            if (bibentry5on) {
                $('#annotation5hider').html('show annotations');
                $('#anno5.bib.annotation').css('display', 'none');
                bibentry5on = false;
            }
            else {
                $('#anno5.bib.annotation').css('display', 'inherit');
                $('#annotation5hider').html("hide annotations");
                bibentry5on = true;
            }
        });
        $('#annotation5hider').click();</script>
<script>
        var bibentry6on = true;
        $('#annotation6hider').click(function() {
            console.trace('here');
            if (bibentry6on) {
                $('#annotation6hider').html('show annotations');
                $('#anno6.bib.annotation').css('display', 'none');
                bibentry6on = false;
            }
            else {
                $('#anno6.bib.annotation').css('display', 'inherit');
                $('#annotation6hider').html("hide annotations");
                bibentry6on = true;
            }
        });
        $('#annotation6hider').click();</script>
<script>
        var bibentry7on = true;
        $('#annotation7hider').click(function() {
            console.trace('here');
            if (bibentry7on) {
                $('#annotation7hider').html('show annotations');
                $('#anno7.bib.annotation').css('display', 'none');
                bibentry7on = false;
            }
            else {
                $('#anno7.bib.annotation').css('display', 'inherit');
                $('#annotation7hider').html("hide annotations");
                bibentry7on = true;
            }
        });
        $('#annotation7hider').click();</script>
<script>
        var bibentry8on = true;
        $('#annotation8hider').click(function() {
            console.trace('here');
            if (bibentry8on) {
                $('#annotation8hider').html('show annotations');
                $('#anno8.bib.annotation').css('display', 'none');
                bibentry8on = false;
            }
            else {
                $('#anno8.bib.annotation').css('display', 'inherit');
                $('#annotation8hider').html("hide annotations");
                bibentry8on = true;
            }
        });
        $('#annotation8hider').click();</script>
<script>
        var bibentry9on = true;
        $('#annotation9hider').click(function() {
            console.trace('here');
            if (bibentry9on) {
                $('#annotation9hider').html('show annotations');
                $('#anno9.bib.annotation').css('display', 'none');
                bibentry9on = false;
            }
            else {
                $('#anno9.bib.annotation').css('display', 'inherit');
                $('#annotation9hider').html("hide annotations");
                bibentry9on = true;
            }
        });
        $('#annotation9hider').click();</script>
<script>
        var bibentry10on = true;
        $('#annotation10hider').click(function() {
            console.trace('here');
            if (bibentry10on) {
                $('#annotation10hider').html('show annotations');
                $('#anno10.bib.annotation').css('display', 'none');
                bibentry10on = false;
            }
            else {
                $('#anno10.bib.annotation').css('display', 'inherit');
                $('#annotation10hider').html("hide annotations");
                bibentry10on = true;
            }
        });
        $('#annotation10hider').click();</script>
<script>
        var bibentry11on = true;
        $('#annotation11hider').click(function() {
            console.trace('here');
            if (bibentry11on) {
                $('#annotation11hider').html('show annotations');
                $('#anno11.bib.annotation').css('display', 'none');
                bibentry11on = false;
            }
            else {
                $('#anno11.bib.annotation').css('display', 'inherit');
                $('#annotation11hider').html("hide annotations");
                bibentry11on = true;
            }
        });
        $('#annotation11hider').click();</script>
<script>
        var bibentry12on = true;
        $('#annotation12hider').click(function() {
            console.trace('here');
            if (bibentry12on) {
                $('#annotation12hider').html('show annotations');
                $('#anno12.bib.annotation').css('display', 'none');
                bibentry12on = false;
            }
            else {
                $('#anno12.bib.annotation').css('display', 'inherit');
                $('#annotation12hider').html("hide annotations");
                bibentry12on = true;
            }
        });
        $('#annotation12hider').click();</script>
<script>
        var bibentry13on = true;
        $('#annotation13hider').click(function() {
            console.trace('here');
            if (bibentry13on) {
                $('#annotation13hider').html('show annotations');
                $('#anno13.bib.annotation').css('display', 'none');
                bibentry13on = false;
            }
            else {
                $('#anno13.bib.annotation').css('display', 'inherit');
                $('#annotation13hider').html("hide annotations");
                bibentry13on = true;
            }
        });
        $('#annotation13hider').click();</script>
<script>
        var bibentry14on = true;
        $('#annotation14hider').click(function() {
            console.trace('here');
            if (bibentry14on) {
                $('#annotation14hider').html('show annotations');
                $('#anno14.bib.annotation').css('display', 'none');
                bibentry14on = false;
            }
            else {
                $('#anno14.bib.annotation').css('display', 'inherit');
                $('#annotation14hider').html("hide annotations");
                bibentry14on = true;
            }
        });
        $('#annotation14hider').click();</script>
<script>
        var bibentry15on = true;
        $('#annotation15hider').click(function() {
            console.trace('here');
            if (bibentry15on) {
                $('#annotation15hider').html('show annotations');
                $('#anno15.bib.annotation').css('display', 'none');
                bibentry15on = false;
            }
            else {
                $('#anno15.bib.annotation').css('display', 'inherit');
                $('#annotation15hider').html("hide annotations");
                bibentry15on = true;
            }
        });
        $('#annotation15hider').click();</script>
<script> var allshowing=false; 
function hideall() { 
allshowing=false;bibentry0on = true; $('#annotation0hider').click();
bibentry1on = true; $('#annotation1hider').click();
bibentry2on = true; $('#annotation2hider').click();
bibentry3on = true; $('#annotation3hider').click();
bibentry4on = true; $('#annotation4hider').click();
bibentry5on = true; $('#annotation5hider').click();
bibentry6on = true; $('#annotation6hider').click();
bibentry7on = true; $('#annotation7hider').click();
bibentry8on = true; $('#annotation8hider').click();
bibentry9on = true; $('#annotation9hider').click();
bibentry10on = true; $('#annotation10hider').click();
bibentry11on = true; $('#annotation11hider').click();
bibentry12on = true; $('#annotation12hider').click();
bibentry13on = true; $('#annotation13hider').click();
bibentry14on = true; $('#annotation14hider').click();
bibentry15on = true; $('#annotation15hider').click();
}
function showall() { 
allshowing=true;
bibentry0on = false; $('#annotation0hider').click();
bibentry1on = false; $('#annotation1hider').click();
bibentry2on = false; $('#annotation2hider').click();
bibentry3on = false; $('#annotation3hider').click();
bibentry4on = false; $('#annotation4hider').click();
bibentry5on = false; $('#annotation5hider').click();
bibentry6on = false; $('#annotation6hider').click();
bibentry7on = false; $('#annotation7hider').click();
bibentry8on = false; $('#annotation8hider').click();
bibentry9on = false; $('#annotation9hider').click();
bibentry10on = false; $('#annotation10hider').click();
bibentry11on = false; $('#annotation11hider').click();
bibentry12on = false; $('#annotation12hider').click();
bibentry13on = false; $('#annotation13hider').click();
bibentry14on = false; $('#annotation14hider').click();
bibentry15on = false; $('#annotation15hider').click();
}
$('#showhide').click(function() {if (allshowing) { hideall(); $('#showhide').html('Expand all annoations');}else { showall(); $('#showhide').html('Hide all annoations');}});</script>
